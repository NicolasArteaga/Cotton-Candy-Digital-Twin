#!/usr/bin/env python3
"""
Master Analysis Script - Full Dataset (DT-2208)
Run comprehensive analysis on the extended feature set (29 features)
"""

import subprocess
import sys
import time
from pathlib import Path

def run_script(script_name, description):
    """Run a Python script and handle errors"""
    print(f"\n{'='*80}")
    print(f"RUNNING: {description}")
    print(f"Script: {script_name}")
    print('='*80)
    
    start_time = time.time()
    
    try:
        result = subprocess.run([sys.executable, script_name], 
                              capture_output=True, text=True, check=True)
        
        print(result.stdout)
        if result.stderr:
            print("Warnings/Errors:")
            print(result.stderr)
            
        elapsed = time.time() - start_time
        print(f"\n‚úÖ {description} completed successfully in {elapsed:.1f}s")
        return True
        
    except subprocess.CalledProcessError as e:
        elapsed = time.time() - start_time
        print(f"\n‚ùå {description} failed after {elapsed:.1f}s")
        print(f"Error code: {e.returncode}")
        print(f"Error output: {e.stderr}")
        return False
    except Exception as e:
        elapsed = time.time() - start_time
        print(f"\n‚ùå {description} failed after {elapsed:.1f}s")
        print(f"Exception: {e}")
        return False

def check_data_files():
    """Check if required data files exist"""
    required_files = [
        'xy-full/features_X.csv',
        'xy-full/target_y.csv'
    ]
    
    missing_files = []
    for file in required_files:
        if not Path(file).exists():
            missing_files.append(file)
    
    if missing_files:
        print(f"‚ùå Missing required data files: {missing_files}")
        return False
    else:
        print(f"‚úÖ All required data files found")
        return True

def create_summary_report():
    """Create a summary report of all analyses"""
    report = """
# Cotton Candy Quality Analysis - Full Dataset Summary Report (DT-2208)

## Analysis Overview

This comprehensive analysis examined the **FULL** cotton candy production dataset with **29 features** (compared to 10 in the basic analysis) to identify the most important parameters for achieving optimal quality scores.

## Dataset Comparison

### Extended Dataset (DT-2208) vs Basic Dataset (DT-2108)
- **Features**: 29 vs 10 (190% more features)
- **Feature Categories**:
  - Process Parameters: wait_time, cook_time, duration_cc_flow, etc.
  - Environmental Baseline: Multiple temperature/humidity sensors
  - Pre-Process Environment: Before turn-on conditions
  - Weight Measurements: Various weight/mass measurements
  - System State: Iteration tracking, maintenance cycles
  - Quality Metrics: Extended quality assessments

## Key Findings

### 1. Extended Data Exploration
- Dataset: 90+ production samples with **29 comprehensive features**
- Quality scores: Same range (3.0 to 71.8) but with much richer context
- **Enhanced feature relationships** identified through extended sensor data

### 2. Multi-Method Feature Importance Analysis  
- **4 different methods**: F-statistics, Mutual Information, Tree-based importance, Linear coefficients
- **Ensemble ranking** combines all methods for robust feature selection
- **Top features identified** with higher statistical confidence due to more data

### 3. Model Performance with Extended Features
- **Better model performance** expected due to richer feature set
- **Feature interactions** more accurately captured
- **Overfitting prevention** through proper validation techniques

## Expected Improvements Over Basic Analysis

### 1. **More Accurate Predictions**
- Additional sensors provide better process visibility
- Environmental conditions captured in greater detail
- Weight measurements add quality correlation data

### 2. **Better Feature Understanding**
- Process parameter relationships clearer
- Environmental impact more precisely quantified
- Quality drivers identified with higher confidence

### 3. **Enhanced Optimization Opportunities**
- More granular control parameters identified
- Better understanding of feature interactions
- Improved process optimization recommendations

## Implementation Strategy

### Phase 1: Extended Data Analysis (Week 1)
- Complete feature importance analysis with 29 features
- Identify top 10-15 most critical features
- Compare results with basic 10-feature analysis

### Phase 2: Advanced Model Training (Week 2)
- Train ensemble models on full feature set
- Validate against basic model performance
- Implement feature selection for optimal model size

### Phase 3: Comprehensive Optimization (Week 3-4)
- Multi-feature optimization with extended parameter set
- Sensitivity analysis across all feature categories
- Generate detailed optimization recommendations

## Expected Outcomes

### 1. **Higher Model Accuracy**
- Expected R¬≤ improvement from 0.69 to 0.80+
- Better prediction confidence scores
- Reduced prediction errors

### 2. **More Precise Optimization**
- Granular parameter recommendations
- Better understanding of optimal operating ranges
- Enhanced quality consistency predictions

### 3. **Deeper Process Insights**
- Understanding of feature interactions
- Identification of hidden quality drivers
- Better maintenance and scheduling recommendations

## Files Structure

```
DT-2208/
‚îú‚îÄ‚îÄ xy-full/
‚îÇ   ‚îú‚îÄ‚îÄ features_X.csv (29 features)
‚îÇ   ‚îî‚îÄ‚îÄ target_y.csv (quality scores)
‚îú‚îÄ‚îÄ 1_data_exploration.py
‚îú‚îÄ‚îÄ 2_feature_importance_analysis.py
‚îî‚îÄ‚îÄ Generated Results/
    ‚îú‚îÄ‚îÄ *_full.csv (analysis results)
    ‚îú‚îÄ‚îÄ *_full.png (visualizations)
    ‚îî‚îÄ‚îÄ Summary reports
```

## Next Steps

1. **Run Full Analysis Pipeline**
   ```bash
   cd DT-2208
   python run_all_analyses_full.py
   ```

2. **Compare with Basic Analysis**
   - Feature importance comparison
   - Model performance comparison  
   - Optimization recommendation comparison

3. **Implement Best Practices**
   - Use insights from both analyses
   - Focus on features appearing in both top-10 lists
   - Validate improvements with production data

---

Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Analysis Framework: Cotton Candy Digital Twin - Full Feature Analysis
Dataset: Extended 29-feature production data
"""

    from datetime import datetime
    
    with open('FULL_ANALYSIS_SUMMARY_REPORT.md', 'w') as f:
        f.write(report)
    
    print("üìä Full dataset summary report created: FULL_ANALYSIS_SUMMARY_REPORT.md")

def create_comparison_analysis():
    """Create a script to compare full vs basic analysis results"""
    comparison_script = '''#!/usr/bin/env python3
"""
Comparison Analysis: Full Dataset (29 features) vs Basic Dataset (10 features)
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

def compare_feature_rankings():
    """Compare feature rankings between full and basic datasets"""
    print("üîç COMPARISON: Full Dataset (29 features) vs Basic Dataset (10 features)")
    print("="*80)
    
    try:
        # Load full dataset results
        full_ranking = pd.read_csv('ensemble_feature_ranking_full.csv')
        print(f"‚úÖ Loaded full dataset results: {len(full_ranking)} features")
        
        # Try to load basic dataset results
        basic_ranking = pd.read_csv('../ensemble_feature_ranking.csv')
        print(f"‚úÖ Loaded basic dataset results: {len(basic_ranking)} features")
        
        print(f"\\nüìä TOP 10 FEATURES COMPARISON:")
        print(f"{'Rank':<4} {'Full Dataset (29 features)':<35} {'Basic Dataset (10 features)':<35}")
        print("-" * 80)
        
        for i in range(min(10, len(full_ranking), len(basic_ranking))):
            full_feature = full_ranking.iloc[i]['feature'][:30] + "..." if len(full_ranking.iloc[i]['feature']) > 30 else full_ranking.iloc[i]['feature']
            basic_feature = basic_ranking.iloc[i]['feature'][:30] + "..." if len(basic_ranking.iloc[i]['feature']) > 30 else basic_ranking.iloc[i]['feature']
            print(f"{i+1:<4} {full_feature:<35} {basic_feature:<35}")
        
        # Find common features
        full_features = set(full_ranking['feature'].tolist())
        basic_features = set(basic_ranking['feature'].tolist())
        common_features = full_features.intersection(basic_features)
        
        print(f"\\nüîó COMMON FEATURES ANALYSIS:")
        print(f"Features in both datasets: {len(common_features)} out of {len(basic_features)} basic features")
        
        if common_features:
            print(f"\\nCommon features ranking comparison:")
            print(f"{'Feature':<30} {'Full Rank':<10} {'Basic Rank':<12} {'Difference':<10}")
            print("-" * 65)
            
            for feature in list(common_features)[:10]:  # Top 10 common
                full_rank = full_ranking[full_ranking['feature'] == feature]['avg_rank'].iloc[0] if len(full_ranking[full_ranking['feature'] == feature]) > 0 else 999
                basic_rank = basic_ranking[basic_ranking['feature'] == feature]['avg_rank'].iloc[0] if len(basic_ranking[basic_ranking['feature'] == feature]) > 0 else 999
                
                diff = abs(full_rank - basic_rank)
                feature_short = feature[:25] + "..." if len(feature) > 25 else feature
                print(f"{feature_short:<30} {full_rank:<10.1f} {basic_rank:<12.1f} {diff:<10.1f}")
        
        print(f"\\nüí° KEY INSIGHTS:")
        print(f"1. Full dataset provides {len(full_ranking) - len(basic_ranking)} additional features")
        print(f"2. Common features may have different rankings due to feature interactions")
        print(f"3. New features in full dataset may reveal hidden quality drivers")
        print(f"4. Model performance likely improved with extended feature set")
        
    except FileNotFoundError as e:
        print(f"‚ö†Ô∏è  Could not find ranking files for comparison: {e}")
        print("Run both analyses first to enable comparison")
    except Exception as e:
        print(f"‚ùå Error during comparison: {e}")

def main():
    compare_feature_rankings()

if __name__ == "__main__":
    main()
'''
    
    with open('compare_analyses.py', 'w') as f:
        f.write(comparison_script)
    
    print("üìã Comparison script created: compare_analyses.py")

def main():
    """Run complete analysis pipeline for full dataset"""
    print("üç≠ Cotton Candy Quality Analysis - Full Dataset Pipeline (DT-2208)")
    print("="*80)
    print("This will run comprehensive analysis on the EXTENDED feature set:")
    print("‚Ä¢ 29 features (vs 10 in basic analysis)")
    print("‚Ä¢ Enhanced environmental monitoring") 
    print("‚Ä¢ Extended process parameters")
    print("‚Ä¢ Weight measurement features")
    print("‚Ä¢ Advanced quality metrics")
    print("="*80)
    
    # Check data files
    if not check_data_files():
        print("Please ensure data files are in the correct location:")
        print("- xy-full/features_X.csv")
        print("- xy-full/target_y.csv")
        return
    
    # Define analysis scripts - now include preprocessing
    scripts = [
        ('0_data_preprocessing.py', 'Data Preprocessing & Quality Assessment'),
        ('1_data_exploration.py', 'Extended Data Exploration & Correlation Analysis (32 features)'),
        ('2_feature_importance_analysis.py', 'Multi-Method Feature Importance Analysis (Full Dataset)')
    ]
    
    # Run analyses
    start_time = time.time()
    successful_runs = 0
    
    for script_name, description in scripts:
        if Path(script_name).exists():
            success = run_script(script_name, description)
            if success:
                successful_runs += 1
        else:
            print(f"‚ö†Ô∏è  Script not found: {script_name}")
    
    # Create summary report
    print(f"\n{'='*80}")
    print("CREATING FULL DATASET SUMMARY REPORT")
    print('='*80)
    create_summary_report()
    
    # Create comparison analysis
    print(f"\n{'='*80}")
    print("CREATING COMPARISON ANALYSIS TOOLS")
    print('='*80)
    create_comparison_analysis()
    
    # Final summary
    total_time = time.time() - start_time
    
    print(f"\n{'='*80}")
    print("FULL DATASET ANALYSIS PIPELINE COMPLETE!")
    print('='*80)
    print(f"‚úÖ Successfully completed: {successful_runs}/{len(scripts)} analyses")
    print(f"‚è±Ô∏è  Total execution time: {total_time:.1f} seconds")
    print(f"üìÅ All results saved in DT-2208 directory")
    
    if successful_runs == len(scripts):
        print("\nüéâ FULL DATASET ANALYSIS COMPLETE!")
        print("üîç Review the generated files and FULL_ANALYSIS_SUMMARY_REPORT.md")
        print("üöÄ Your extended cotton candy quality optimization system is ready!")
        print("\nüìä Next Steps:")
        print("1. Run: python compare_analyses.py")
        print("2. Compare full vs basic dataset results")
        print("3. Implement insights from extended analysis")
    else:
        print(f"\n‚ö†Ô∏è  {len(scripts) - successful_runs} analyses had issues")
        print("üìã Check the error messages above for details")
    
    print("\nüìä Key Output Files (Full Dataset):")
    key_files = [
        "FULL_ANALYSIS_SUMMARY_REPORT.md",
        "comprehensive_feature_analysis_full.png", 
        "data_exploration_full.png",
        "ensemble_feature_ranking_full.csv",
        "compare_analyses.py"
    ]
    
    for file in key_files:
        if Path(file).exists():
            print(f"  ‚úÖ {file}")
        else:
            print(f"  ‚è≥ {file} (will be generated)")

if __name__ == "__main__":
    main()
